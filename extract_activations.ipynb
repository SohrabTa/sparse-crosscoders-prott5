{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# ProtT5 Activation Extraction\n",
                "\n",
                "This notebook loads the `Rostlab/prot_t5_xl_half_uniref50-enc` model, fetches a random sequence from UniRef50 (filtered by date and length), and extracts its hidden activations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import T5EncoderModel, T5Tokenizer\n",
                "import requests\n",
                "import random\n",
                "import re\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# 1. GPU Check\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    device = torch.device(\"cuda\")\n",
                "elif torch.backends.mps.is_available():\n",
                "    print(\"Using Apple Silicon (MPS)\")\n",
                "    device = torch.device(\"mps\")\n",
                "else:\n",
                "    raise RuntimeError(\"No GPU (CUDA or MPS) found! This script requires a valid accelerator for efficient inference.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model-loading",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Model and Tokenizer\n",
                "model_name = 'Rostlab/prot_t5_xl_half_uniref50-enc'\n",
                "print(f\"Loading model: {model_name}...\")\n",
                "\n",
                "tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)\n",
                "model = T5EncoderModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "print(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-fetching",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Fetch Random UniRef50 Sequence\n",
                "def get_random_uniref50_sequence():\n",
                "    # Filter: Modified before 2019, length <= 512\n",
                "    # UniRef search API\n",
                "    url = \"https://rest.uniprot.org/uniref/search\"\n",
                "    query = \"date_modified:[* TO 2019-01-01] AND length:[1 TO 512] identity:0.5\"\n",
                "    \n",
                "    params = {\n",
                "        'query': query,\n",
                "        'format': 'json',\n",
                "        'size': 50  # Fetch a batch\n",
                "    }\n",
                "    \n",
                "    print(\"Fetching random sequence from UniRef50...\")\n",
                "    response = requests.get(url, params=params)\n",
                "    response.raise_for_status()\n",
                "    data = response.json()\n",
                "    \n",
                "    if 'results' not in data or not data['results']:\n",
                "        raise ValueError(\"No results found for the query.\")\n",
                "    \n",
                "    # Pick a random entry\n",
                "    entry = random.choice(data['results'])\n",
                "    \n",
                "    # Extract sequence\n",
                "    try:\n",
                "        sequence = entry['representativeMember']['sequence']['value']\n",
                "        accession = entry['id']\n",
                "        print(f\"Selected UniRef50 Entry: {accession}\")\n",
                "        print(f\"Sequence Length: {len(sequence)}\")\n",
                "        return sequence\n",
                "    except KeyError:\n",
                "        # Fallback if structure is different\n",
                "        print(\"Could not parse sequence from entry, dumping keys:\", entry.keys())\n",
                "        raise\n",
                "\n",
                "sequence = get_random_uniref50_sequence()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "extraction",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Extract Activations\n",
                "\n",
                "# Pre-processing (Regex replace UZOB -> X, add spaces)\n",
                "processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
                "\n",
                "# Tokenize\n",
                "ids = tokenizer.batch_encode_plus([processed_seq], add_special_tokens=True, padding=\"longest\")\n",
                "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
                "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
                "\n",
                "print(f\"Input IDs shape: {input_ids.shape}\")\n",
                "\n",
                "# Forward Pass\n",
                "print(\"Running inference...\")\n",
                "with torch.no_grad():\n",
                "    output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
                "\n",
                "# Extract Hidden States (excluding the last one)\n",
                "# output.hidden_states is a tuple of (embedding_output, layer_1, ..., layer_N)\n",
                "# We want all except the very last one (which is the final encoder output)\n",
                "all_hidden_states = output.hidden_states[:-1]\n",
                "\n",
                "# Stack them: (num_layers, batch_size, seq_len, hidden_dim)\n",
                "stacked_activations = torch.stack(all_hidden_states)\n",
                "\n",
                "# Remove batch dimension (since batch_size=1)\n",
                "final_activations = stacked_activations.squeeze(1)\n",
                "\n",
                "print(f\"Extracted activations shape: {final_activations.shape}\")\n",
                "print(\"(Layers, Sequence_Length, Hidden_Dim)\")\n",
                "print(\"Note: Layers includes the initial embedding layer.\")\n",
                "\n",
                "# 5. Save\n",
                "save_path = \"random_protein_activations.pt\"\n",
                "torch.save(final_activations, save_path)\n",
                "print(f\"Saved activations to {save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
