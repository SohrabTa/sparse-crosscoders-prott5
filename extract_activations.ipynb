{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# ProtT5 Activation Extraction\n",
                "\n",
                "This notebook loads the `Rostlab/prot_t5_xl_half_uniref50-enc` model, fetches a random sequence from UniRef50 (filtered by date and length), and extracts its hidden activations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using Apple Silicon (MPS)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import T5EncoderModel, T5Tokenizer\n",
                "import requests\n",
                "import random\n",
                "import re\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# 1. GPU Check\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    device = torch.device(\"cuda\")\n",
                "elif torch.backends.mps.is_available():\n",
                "    print(\"Using Apple Silicon (MPS)\")\n",
                "    device = torch.device(\"mps\")\n",
                "else:\n",
                "    raise RuntimeError(\"No GPU (CUDA or MPS) found! This script requires a valid accelerator for efficient inference.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "model-loading",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model: Rostlab/prot_t5_xl_half_uniref50-enc...\n",
                        "Model loaded successfully.\n"
                    ]
                }
            ],
            "source": [
                "# 2. Load Model and Tokenizer\n",
                "model_name = 'Rostlab/prot_t5_xl_half_uniref50-enc'\n",
                "print(f\"Loading model: {model_name}...\")\n",
                "\n",
                "tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)\n",
                "model = T5EncoderModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "print(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "data-fetching",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fetching random sequences (batch_size=5) from UniRef50...\n",
                        "Selected UniRef50 Entry: UniRef50_A0A010QHV5, Length: 85\n",
                        "Selected UniRef50 Entry: UniRef50_A0A010R7Z2, Length: 225\n",
                        "Selected UniRef50 Entry: UniRef50_A0A009R5V4, Length: 54\n",
                        "Selected UniRef50 Entry: UniRef50_A0A010RC96, Length: 278\n",
                        "Selected UniRef50 Entry: UniRef50_A0A007, Length: 407\n"
                    ]
                }
            ],
            "source": [
                "# 3. Fetch Random UniRef50 Sequences\n",
                "def get_random_uniref50_sequences(batch_size=5):\n",
                "    # Filter: Modified before 2019, length <= 512\n",
                "    # UniRef search API\n",
                "    url = \"https://rest.uniprot.org/uniref/search\"\n",
                "    query = \"date_modified:[* TO 2019-01-01] AND length:[1 TO 512] identity:0.5\"\n",
                "    \n",
                "    params = {\n",
                "        'query': query,\n",
                "        'format': 'json',\n",
                "        'size': 50  # Fetch a larger pool to sample from\n",
                "    }\n",
                "    \n",
                "    print(f\"Fetching random sequences (batch_size={batch_size}) from UniRef50...\")\n",
                "    response = requests.get(url, params=params)\n",
                "    response.raise_for_status()\n",
                "    data = response.json()\n",
                "    \n",
                "    if 'results' not in data or not data['results']:\n",
                "        raise ValueError(\"No results found for the query.\")\n",
                "    \n",
                "    # Pick random entries\n",
                "    entries = random.sample(data['results'], min(batch_size, len(data['results'])))\n",
                "    \n",
                "    sequences = []\n",
                "    for entry in entries:\n",
                "        # Extract sequence\n",
                "        try:\n",
                "            seq = entry['representativeMember']['sequence']['value']\n",
                "            accession = entry['id']\n",
                "            print(f\"Selected UniRef50 Entry: {accession}, Length: {len(seq)}\")\n",
                "            sequences.append(seq)\n",
                "        except KeyError:\n",
                "            # Fallback if structure is different\n",
                "            print(\"Could not parse sequence from entry, dumping keys:\", entry.keys())\n",
                "            continue\n",
                "            \n",
                "    return sequences\n",
                "\n",
                "sequences = get_random_uniref50_sequences(batch_size=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "extraction",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input IDs shape: torch.Size([5, 408])\n",
                        "Running inference...\n",
                        "Extracted activations shape: torch.Size([24, 5, 408, 1024])\n",
                        "(Layers, Batch_Size, Sequence_Length, Hidden_Dim)\n",
                        "Note: Layers includes the initial embedding layer.\n",
                        "Saved activations to random_protein_activations_batch.pt\n"
                    ]
                }
            ],
            "source": [
                "# 4. Extract Activations\n",
                "\n",
                "# Pre-processing (Regex replace UZOB -> X, add spaces)\n",
                "processed_seqs = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq))) for seq in sequences]\n",
                "\n",
                "# Tokenize\n",
                "ids = tokenizer.batch_encode_plus(processed_seqs, add_special_tokens=True, padding=\"longest\")\n",
                "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
                "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
                "\n",
                "print(f\"Input IDs shape: {input_ids.shape}\")\n",
                "\n",
                "# Forward Pass\n",
                "print(\"Running inference...\")\n",
                "with torch.no_grad():\n",
                "    output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
                "\n",
                "# Extract Hidden States (excluding the last one)\n",
                "# output.hidden_states is a tuple of (embedding_output, layer_1, ..., layer_N)\n",
                "# We want all except the very last one (which is the final encoder output)\n",
                "all_hidden_states = output.hidden_states[:-1]\n",
                "\n",
                "# Stack them: (num_layers, batch_size, seq_len, hidden_dim)\n",
                "stacked_activations = torch.stack(all_hidden_states)\n",
                "\n",
                "# Verify shape\n",
                "print(f\"Extracted activations shape: {stacked_activations.shape}\")\n",
                "print(\"(Layers, Batch_Size, Sequence_Length, Hidden_Dim)\")\n",
                "print(\"Note: Layers includes the initial embedding layer.\")\n",
                "\n",
                "# 5. Save\n",
                "save_path = \"random_protein_activations_batch.pt\"\n",
                "torch.save(stacked_activations, save_path)\n",
                "print(f\"Saved activations to {save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
